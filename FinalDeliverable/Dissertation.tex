\documentclass[11pt]{report}
\usepackage[table]{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage{setspace}
\doublespacing

\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{inputenc}

\usepackage{dashrule}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{mwe}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{minted}
\usepackage{tabularx}
%\usepackage{ltablex}
\usepackage{enumitem}
\usepackage{ragged2e}
\usepackage{xcolor}
\usepackage{subfig}

\usepackage[backend=biber,style=ieee]{biblatex}
\usepackage[toc]{appendix}
\usepackage[acronym]{glossaries}

\addbibresource{Dissertation.bib}

\hypersetup{ linktoc=all}
\graphicspath{ {./images/}{../images/}{../../images}{../Deliverable_1/images}{../../Deliverable_1/images} }

\makeatletter
\newcommand*{\compress}{\@minipagetrue}
\makeatother

\makenoidxglossaries

\newacronym{ai}{AI}{artificial intelligence}
\newacronym{fc}{FC}{fully connected}
\newacronym{dnn}{DNN}{deep neural network}
\newacronym{cnn}{CNN}{convolutional neural network}
\newacronym{rnn}{RNN}{recurrent neural network}
\newacronym{lstm}{LSTM}{long term short term memory network}
\newacronym{nlp}{NLP}{natural language processing}

\newacronym{ncs}{NCS}{neural compute stick}
\newacronym{tpu}{TPU}{tensor processing unit}
\newacronym{vpu}{VPU}{video processing unit}
\newacronym{gpu}{GPU}{graphics processing unit}
\newacronym{apu}{APU}{associative processing unit}
\newacronym{fpga}{FPGA}{field programmable gate array}
\newacronym{asic}{ASIC}{application specific integrated circuit}

\newacronym{blas}{BLAS}{basic linear algebra subprograms}
\newacronym{csc}{CSC}{compressed sparse column}
\newacronym{tops}{TOPS}{trillion operations per second}
\newacronym{soc}{SoC}{system on a chip}

\newacronym{wandb}{WandB}{Weights and Biases}


\definecolor{mintedgreen}{RGB}{44, 126, 53}

\usepackage{subfiles}

\begin{document}
\title{%
	\bf Inference at the edge: Tuning Compression Parameters for Performance\\ 
	\large Dissertation \\
	BSc Computer Science: Artificial Intelligence}

\author{
	Sam Fay-Hunt | \texttt{sf52@hw.ac.uk}\\
	Supervisor: Rob Stewart | \texttt{R.Stewart@hw.ac.uk}
}

\maketitle
\thispagestyle{empty}
\pagebreak

\textbf{DECLARATION}\\
I, Sam Fay-Hunt confirm that this work submitted for assessment is my own and is expressed in
my own words. Any uses made within it of the works of other authors in any form (e.g., ideas,
equations, figures, text, tables, programs) are properly acknowledged at any point of their
use. A list of the references employed is included.\\
Signed: .....Sam Fay-Hunt...........\\
Date: .....21/04/2020...............
\thispagestyle{empty}
\pagebreak

\textbf{Abstract:} 
\emph{Abstract here}



\thispagestyle{empty}
\pagebreak

\tableofcontents
\thispagestyle{empty}
\pagebreak

\printnoidxglossary[type=acronym, nonumberlist]
\thispagestyle{empty}

\newpage
\setcounter{page}{1}

\chapter{Introduction}
\subfile{Sections/introduction}



\pagebreak
\chapter{Background}
\emph{\color{red}
\begin{itemize}
	\item Adapt from D1
	\item rewrite with more of a focus on the concrete channel and pruning methodology used
	\item Would be good to include wandb bayes hyperparam optimisation details
	\item Discuss filters and channels in more depth
\end{itemize}
}
This Chapter is split into 4 sections:\\
Section~\ref{subsec:deepLearning} | \textbf{Deep Learning}: An overview of the basic components of a deep neural network and the \acrshort{cnn} model.\\
Section~\ref{subsec:compressionTypes} | \textbf{Neural Network Compression}: Discusses neural network compression techniques and on how they change the underlying representations of DNNs.\\
Section~\ref{subsec:AIaccelerators} | \textbf{AI accelerators} Covers a few popular AI accelerators architectures, their strengths, weaknesses and specialisms.\\
Section~\ref{subsec:hardwareArch} | \textbf{Memory factors for Deep Neural Networks}: Describes how DNNs interact with memory, and discusses some of the implications of this.

\section{Deep Neural Networks}\label{subsec:deepLearning}
\subfile{Sections/Background/DeepNeuralNetworks}

\newpage
\section{Neural Network Compression}\label{subsec:compressionTypes}
\subfile{Sections/Background/Compression}

\newpage
\section{AI accelerators}\label{subsec:AIaccelerators}
\subfile{Sections/Background/AIaccelerators}

\newpage
\section{Memory factors for Deep Neural Networks}\label{subsec:hardwareArch}
\subfile{Sections/Background/HardwareMemArch}

\newpage



\chapter{Methodology}
\subfile{Sections/Methodology}


\chapter{Evaluation}
\subfile{Sections/Evaluation}

\chapter{Conclusion}
\subfile{Sections/Conclusion}

\newpage
\appendix
\chapter{Back matter}
\section{Model Listing}
The following section is a listing of cherry-picked models, with a hyperlink to further details for each model, and a description of the Pruning parameters and observed metrics.

All networks in this section have links to the original data as it was gathered, the pruning schedule labels used in this data are different to those described throughout the dissertation. 
The label names were changed to improve the readability of this dissertation, Table~\ref{tab:labelMapping} shows how the labels in this dissertation map to the labels in the original data. 


\begin{table}[H]
    \centering
    \begin{tabular}{@{}lll@{}}
    \toprule
    \textbf{Dissertation Label} &                & \textbf{Label in Dataset} \\ \hline
    Filter Pruner Layer 1       & $\rightarrow$ & filter\_pruner\_70        \\
    Filter Pruner Layer 2       & $\rightarrow$ & filter\_pruner\_60        \\
    Filter Pruner Layer 3.1     & $\rightarrow$ & filter\_pruner\_20        \\
    Filter Pruner Layer 3.2     & $\rightarrow$ & filter\_pruner\_40        \\ \hline
    \end{tabular}
    \caption{Mapping of labels from dissertation to dataset}
    \label{tab:labelMapping}
\end{table}


\subsection*{\protect\href{https://wandb.ai/samfh/Resnet56-Filters-Test/runs/eje5tk6m/overview?workspace=}{\underline{\color{blue}Golden-sweep-523}}}\label{sec:golden-sweep-523}
This model was created during the `no-retraining' experiment, very light pruning parameters with a low Top1, and a latency similar to no pruning. We can see how just a small amount of pruning has a dramatic effect on the accuracy.\\
Referenced by: [\ref{sec:FastPruningPhase}]
\begin{figure}[H]
    \begin{table}[H]
        \centering
        \subfloat[Pruning parameters]{
            \begin{tabular}{@{}ll@{}}
                \toprule
                \textbf{Parameter Name} & \textbf{Value} \\ \midrule
                filter\_pruner\_20      & 0.153          \\
                filter\_pruner\_40      & 0.2536         \\
                filter\_pruner\_60      & 0.09001        \\
                filter\_pruner\_70      & 0.09955        \\ \bottomrule
            \end{tabular}
        }
        \hspace{2em}
        \quad
        \subfloat[Recorded Metrics]{
            \begin{tabular}{@{}ll@{}}
                \toprule
                \textbf{Metric}    & \textbf{Value} \\ \midrule
                Latency(ms)        & 4.397          \\
                Loss               & 1.924          \\
                Throughput(FPS)    & 302.36         \\
                Top1               & 27.02          \\
                Top5               & 83.26          \\
                Total\_latency(ms) & 13.06          \\ \bottomrule
            \end{tabular}
        }
        %\setcounter{subfloat}{0}
    \end{table}    
\end{figure}


\subsection*{\protect\href{https://wandb.ai/samfh/Resnet56-Filters-Test/runs/58wpdzct/overview?workspace=}{\underline{\color{blue}comfy-sweep-1007}}}\label{sec:comfy-sweep-1007}
This model was created during the `no-retraining' experiment, high desired pruning parameters with a high (untrained) Top1, and fairly low Latency. \\
Referenced by: [\ref{sec:FastPruningPhase}]
\begin{figure}[H]
    \begin{table}[H]
        \centering
        \subfloat[Pruning parameters]{
            \begin{tabular}{@{}ll@{}}
                \toprule
                \textbf{Parameter Name} & \textbf{Value} \\ \midrule
                filter\_pruner\_20      & 0.9403          \\
                filter\_pruner\_40      & 0.9687         \\
                filter\_pruner\_60      & 0.3814        \\
                filter\_pruner\_70      & 0.9707        \\ \bottomrule
            \end{tabular}
        }
        \hspace{2em}
        \subfloat[Recorded Metrics]{
            \begin{tabular}{@{}ll@{}}
                \toprule
                \textbf{Metric}    & \textbf{Value} \\ \midrule
                Latency(ms)        & 3.347          \\
                Loss               & 1.608          \\
                Throughput(FPS)    & 299.94         \\
                Top1               & 39.32          \\
                Top5               & 89.32          \\
                Total\_latency(ms) & 12.85          \\ \bottomrule
            \end{tabular}
        }
        %\setcounter{subfloat}{0}
    \end{table}    
\end{figure}


\subsection*{\protect\href{https://wandb.ai/samfh/Resnet56-Channels-Filters/runs/ptmmklr8?workspace=}{\underline{\color{blue}unique-sweep-182}}}\label{sec:unique-sweep-182}
This model was created during the second experiment targeting latency with retraining, it achieved an impressively low final latency, and managed to go from 3.529ms before retraining to 3.091ms after.\\
Referenced by: [\ref{sec:Experiment2}, \ref{sec:vsBaseline}]
\begin{figure}[H]
    \begin{table}[H]
        \centering
        \subfloat[Pruning parameters]{
            \begin{tabular}{@{}ll@{}}
                \toprule
                \textbf{Parameter Name} & \textbf{Value} \\ \midrule
                filter\_pruner\_20      & 0.4657          \\
                filter\_pruner\_40      & 0.8789         \\
                filter\_pruner\_60      & 0.9352        \\
                filter\_pruner\_70      & 0.9519        \\ \bottomrule
            \end{tabular}
        }
        \hspace{2em}
        \subfloat[Recorded Metrics]{
            \begin{tabular}{@{}ll@{}}
                \toprule
                \textbf{Metric}    & \textbf{Value} \\ \midrule
                Latency(ms)        & 3.091          \\
                Loss               & 0.539          \\
                Throughput(FPS)    & 300.58         \\
                Top1               & 85.48          \\
                Top5               & 99.36          \\
                Total\_latency(ms) & 12.78          \\ \bottomrule
            \end{tabular}
        }
        %\setcounter{subfloat}{0}
    \end{table}    
\end{figure}


\subsection*{\protect\href{https://wandb.ai/samfh/Resnet56-Channels-Filters/runs/drh6012i/overview?workspace=}{\underline{\color{blue}rural-sweep-89}}}\label{sec:rural-sweep-89}
This model was created during the third experiment targeting Top1 with retraining, it achieved a solid balance of Top1 and latency, one of the best all round networks we created.\\
Referenced by: [\ref{sec:vsBaseline}]
\begin{figure}[H]
    \begin{table}[H]
        \centering
        \subfloat[Pruning parameters]{
            \begin{tabular}{@{}ll@{}}
                \toprule
                \textbf{Parameter Name} & \textbf{Value} \\ \midrule
                filter\_pruner\_20      & 0.6756         \\
                filter\_pruner\_40      & 0.1964         \\
                filter\_pruner\_60      & 0.3058         \\
                filter\_pruner\_70      & 0.8031         \\ \bottomrule
            \end{tabular}
        }
        \hspace{2em}
        \subfloat[Recorded Metrics]{
            \begin{tabular}{@{}ll@{}}
                \toprule
                \textbf{Metric}    & \textbf{Value} \\ \midrule
                Latency(ms)        & 3.557          \\
                Loss               & 0.533          \\
                Throughput(FPS)    & 302.02         \\
                Top1               & 88.08          \\
                Top5               & 99.66          \\
                Total\_latency(ms) & 13.07          \\ \bottomrule
            \end{tabular}
        }
        %\setcounter{subfloat}{0}
    \end{table}    
\end{figure}

\subsection*{\protect\href{https://wandb.ai/samfh/Resnet56-Filters-Test/runs/a09xglnd/overview?workspace=}{\underline{\color{blue}fiery-sweep-333}}}\label{sec:fiery-sweep-333}
This model was created outside of the experiments described in the dissertation, it is a good demonstration of the performance available with a higher number of epochs, its best Top1 is on epoch 257, there may be room for further improvement from training. \\
Referenced by: [\ref{sec:vsBaseline}]
\begin{figure}[H]
    \begin{table}[H]
        \centering
        \subfloat[Pruning parameters]{
            \begin{tabular}{@{}ll@{}}
                \toprule
                \textbf{Parameter Name} & \textbf{Value} \\ \midrule
                epochs                  & 285            \\
                learning\_rate          & 0.3013         \\
                filter\_pruner\_20      & 0.6756         \\
                filter\_pruner\_40      & 0.1964         \\
                filter\_pruner\_60      & 0.3058         \\
                filter\_pruner\_70      & 0.8031         \\ \bottomrule
            \end{tabular}
        }
        \hspace{2em}
        \subfloat[Recorded Metrics]{
            \begin{tabular}{@{}ll@{}}
                \toprule
                \textbf{Metric}    & \textbf{Value} \\ \midrule
                Latency(ms)        & 3.265          \\
                Loss               & 0.521          \\
                Throughput(FPS)    & 304.52         \\
                Top1               & 88.04          \\
                Top5               & 99.48          \\
                Total\_latency(ms) & 12.92          \\ \bottomrule
            \end{tabular}
        }
        %\setcounter{subfloat}{0}
    \end{table}    
\end{figure}

\newpage
\section{Schedule}\label{apx:Schedule}
The following is a listing of the `off the shelf' schedule from Distiller. This is a hand written schedule discussed in Section~\textbf{TBD}. 
\singlespacing
\begin{minted}[breaklines, linenos]{yaml}
version: 1
pruners:
  filter_pruner_70:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.7
    weights: [
      module.layer1.0.conv1.weight,
      module.layer1.1.conv1.weight,
      module.layer1.2.conv1.weight,
      module.layer1.3.conv1.weight,
      module.layer1.4.conv1.weight,
      module.layer1.5.conv1.weight,
      module.layer1.6.conv1.weight,
      module.layer1.7.conv1.weight,
      module.layer1.8.conv1.weight]

  filter_pruner_60:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.6
    weights: [
      module.layer2.1.conv1.weight,
      module.layer2.2.conv1.weight,
      module.layer2.3.conv1.weight,
      module.layer2.4.conv1.weight,
      module.layer2.6.conv1.weight,
      module.layer2.7.conv1.weight]

  filter_pruner_20:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.2
    weights: [module.layer3.1.conv1.weight]

  filter_pruner_40:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.4
    weights: [
      module.layer3.2.conv1.weight,
      module.layer3.3.conv1.weight,
      module.layer3.5.conv1.weight,
      module.layer3.6.conv1.weight,
      module.layer3.7.conv1.weight,
      module.layer3.8.conv1.weight]


extensions:
  net_thinner:
      class: 'FilterRemover'
      thinning_func_str: remove_filters
      arch: 'resnet56_cifar'
      dataset: 'cifar10'

lr_schedulers:
   exp_finetuning_lr:
     class: ExponentialLR
     gamma: 0.95


policies:
  - pruner:
      instance_name: filter_pruner_70
    epochs: [0]

  - pruner:
      instance_name: filter_pruner_60
    epochs: [0]

  - pruner:
      instance_name: filter_pruner_40
    epochs: [0]

  - pruner:
      instance_name: filter_pruner_20
    epochs: [0]

  - extension:
      instance_name: net_thinner
    epochs: [0]

  - lr_scheduler:
      instance_name: exp_finetuning_lr
    starting_epoch: 10
    ending_epoch: 300
    frequency: 1

\end{minted}
\doublespacing

\printbibliography


\end{document}
