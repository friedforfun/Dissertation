\documentclass[11pt]{article}
\usepackage[table]{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage{setspace}
\doublespacing

\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{inputenc}

\usepackage{dashrule}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{mwe}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{minted}
\usepackage{tabularx}
%\usepackage{ltablex}
\usepackage{enumitem}
\usepackage{ragged2e}
\usepackage{xcolor}
\usepackage{subfig}

\usepackage[backend=biber,style=ieee]{biblatex}
\usepackage[toc]{appendix}
\usepackage[acronym]{glossaries}

\addbibresource{Dissertation.bib}

\hypersetup{ linktoc=all}
\graphicspath{ {./images/}{../images/}{../../images}{../Deliverable_1/images}{../../Deliverable_1/images} }

\makeatletter
\newcommand*{\compress}{\@minipagetrue}
\makeatother

\makenoidxglossaries

\newacronym{ai}{AI}{artificial intelligence}
\newacronym{fc}{FC}{fully connected}
\newacronym{dnn}{DNN}{deep neural network}
\newacronym{cnn}{CNN}{convolutional neural network}
\newacronym{rnn}{RNN}{recurrent neural network}
\newacronym{lstm}{LSTM}{long term short term memory network}
\newacronym{nlp}{NLP}{natural language processing}

\newacronym{ncs}{NCS}{neural compute stick}
\newacronym{tpu}{TPU}{tensor processing unit}
\newacronym{vpu}{VPU}{video processing unit}
\newacronym{gpu}{GPU}{graphics processing unit}
\newacronym{apu}{APU}{associative processing unit}
\newacronym{fpga}{FPGA}{field programmable gate array}
\newacronym{asic}{ASIC}{application specific integrated circuit}

\newacronym{blas}{BLAS}{basic linear algebra subprograms}
\newacronym{csc}{CSC}{compressed sparse column}
\newacronym{tops}{TOPS}{trillion operations per second}
\newacronym{soc}{SoC}{system on a chip}


\definecolor{mintedgreen}{RGB}{44, 126, 53}

\usepackage{subfiles}

\begin{document}
\title{%
	\bf Inference at the edge: tuning compression parameters for performance\\ 
	\large Deliverable 1: Final year Dissertation \\
	BSc Computer Science: Artificial Intelligence}

\author{
	Sam Fay-Hunt | \texttt{sf52@hw.ac.uk}\\
	Supervisor: Rob Stewart | \texttt{R.Stewart@hw.ac.uk}
}

\maketitle
\thispagestyle{empty}
\pagebreak

\textbf{DECLARATION}\\
I, Sam Fay-Hunt confirm that this work submitted for assessment is my own and is expressed in
my own words. Any uses made within it of the works of other authors in any form (e.g., ideas,
equations, figures, text, tables, programs) are properly acknowledged at any point of their
use. A list of the references employed is included.\\
Signed: .....Sam Fay-Hunt...........\\
Date: .....21/04/2020...............
\thispagestyle{empty}
\pagebreak

\textbf{Abstract:} 
\emph{Abstract here}



\thispagestyle{empty}
\pagebreak

\tableofcontents
\thispagestyle{empty}
\pagebreak

\printnoidxglossary[type=acronym, nonumberlist]
\thispagestyle{empty}

\newpage
\setcounter{page}{1}

\section{Introduction}
\subfile{Sections/introduction}

\emph{
\begin{itemize}
	\item Introduce terminology  - Inference, neural network model, pruning, layers, channels, filters
	\item Introduce models to be used - high level conceptual representation of the models
	\item Introduce hypothesis
	\item Describe research aims
	\item Define project objectives
	\item Describe how this work contributes to further research
\end{itemize}
}


\pagebreak
\section{Background}
\emph{
\begin{itemize}
	\item Adapt from D1
	\item rewrite with more of a focus on the concrete channel and pruning methodology used
	\item Would be good to include wandb bayse hyperparam optimisation details
\end{itemize}
}
This Section will be split into 4 subsections:\\
Section~\ref{subsec:deepLearning} - \textbf{Deep Learning}: An overview of the basic components of a deep neural network and the \acrshort{cnn} model.\\
Section~\ref{subsec:compressionTypes} - \textbf{Neural Network Compression}: Discusses neural network compression techniques and on how they change the underlying representations of DNNs.\\
Section~\ref{subsec:AIaccelerators} - \textbf{AI accelerators} Covers a few popular AI accelerators architectures, their strengths, weaknesses and specialisms.\\
Section~\ref{subsec:hardwareArch} - \textbf{Memory factors for Deep Neural Networks}: Describes how DNNs interact with memory, and discusses some of the implications of this.

\subsection{Deep Neural Networks}\label{subsec:deepLearning}
\subfile{Sections/Background/DeepNeuralNetworks}

\newpage
\subsection{Neural Network Compression}\label{subsec:compressionTypes}
\subfile{Sections/Background/Compression}

\newpage
\subsection{AI accelerators}\label{subsec:AIaccelerators}
\subfile{Sections/Background/AIaccelerators}

\newpage
\subsection{Memory factors for Deep Neural Networks}\label{subsec:hardwareArch}
\subfile{Sections/Background/HardwareMemArch}

\newpage



\section{Methodology}
\subfile{Sections/Methodology}


\section{Evaluation}
\subfile{Sections/Evaluation}

\section{Conclusion}
\subfile{Sections/Conclusion}

\newpage
\appendix
\section{Back matter}
\subsection{Model Listing}
The following section is a listing of cherry-picked models, with a hyperlink to further details for each model, and a description of the Pruning parameters and observed metrics.

All networks in this section have links to the original data as it was gathered, the pruning schedule labels used in this data are different to those described throughout the dissertation. 
The label names were changed to improve the readability of this dissertation, Table~\ref{tab:labelMapping} shows how the labels in this dissertation map to the labels in the original data. 


\begin{table}[H]
    \centering
    \begin{tabular}{@{}lll@{}}
    \toprule
    \textbf{Dissertation Label} &                & \textbf{Label in Dataset} \\ \hline
    Filter Pruner Layer 1       & $\rightarrow$ & filter\_pruner\_20        \\
    Filter Pruner Layer 2       & $\rightarrow$ & filter\_pruner\_40        \\
    Filter Pruner Layer 3.1     & $\rightarrow$ & filter\_pruner\_60        \\
    Filter Pruner Layer 3.2     & $\rightarrow$ & filter\_pruner\_70        \\ \hline
    \end{tabular}
    \caption{Mapping of labels from dissertation to dataset}
    \label{tab:labelMapping}
\end{table}


\subsubsection*{\protect\href{https://wandb.ai/samfh/Resnet56-Filters-Test/runs/eje5tk6m/overview?workspace=}{\underline{\color{blue}Golden-sweep-523}}}\label{sec:golden-sweep-523}
This model was created during the `no-retraining' experiment, very light pruning parameters with a low Top1, and fairly low Latency. We can see how just a small amount of pruning has a dramatic effect on the accuracy.\\
Referenced by: [\ref{sec:FastPruningPhase}]
\begin{table}[H]
    \centering
    \subfloat[Pruning parameters]{
        \begin{tabular}{@{}ll@{}}
            \toprule
            \textbf{Parameter Name} & \textbf{Value} \\ \midrule
            filter\_pruner\_20      & 0.153          \\
            filter\_pruner\_40      & 0.2536         \\
            filter\_pruner\_60      & 0.09001        \\
            filter\_pruner\_70      & 0.09955        \\ \bottomrule
        \end{tabular}
    }
    \hspace{2em}
    \subfloat[Recorded Metrics]{
        \begin{tabular}{@{}ll@{}}
            \toprule
            \textbf{Metric}    & \textbf{Value} \\ \midrule
            Latency(ms)        & 4.397          \\
            Loss               & 1.924          \\
            Throughput(FPS)    & 302.36         \\
            Top1               & 27.02          \\
            Top5               & 83.26          \\
            Total\_latency(ms) & 13.06          \\ \bottomrule
        \end{tabular}
    }
\end{table}


\newpage
\subsection{Schedule}\label{apx:Schedule}
The following is a listing of the `off the shelf' schedule from Distiller. This is a hand written schedule discussed in Section~\textbf{TBD}. 
\singlespacing
\begin{minted}[breaklines, linenos]{yaml}
version: 1
pruners:
  filter_pruner_70:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.7
    weights: [
      module.layer1.0.conv1.weight,
      module.layer1.1.conv1.weight,
      module.layer1.2.conv1.weight,
      module.layer1.3.conv1.weight,
      module.layer1.4.conv1.weight,
      module.layer1.5.conv1.weight,
      module.layer1.6.conv1.weight,
      module.layer1.7.conv1.weight,
      module.layer1.8.conv1.weight]

  filter_pruner_60:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.6
    weights: [
      module.layer2.1.conv1.weight,
      module.layer2.2.conv1.weight,
      module.layer2.3.conv1.weight,
      module.layer2.4.conv1.weight,
      module.layer2.6.conv1.weight,
      module.layer2.7.conv1.weight]

  filter_pruner_20:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.2
    weights: [module.layer3.1.conv1.weight]

  filter_pruner_40:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.4
    weights: [
      module.layer3.2.conv1.weight,
      module.layer3.3.conv1.weight,
      module.layer3.5.conv1.weight,
      module.layer3.6.conv1.weight,
      module.layer3.7.conv1.weight,
      module.layer3.8.conv1.weight]


extensions:
  net_thinner:
      class: 'FilterRemover'
      thinning_func_str: remove_filters
      arch: 'resnet56_cifar'
      dataset: 'cifar10'

lr_schedulers:
   exp_finetuning_lr:
     class: ExponentialLR
     gamma: 0.95


policies:
  - pruner:
      instance_name: filter_pruner_70
    epochs: [0]

  - pruner:
      instance_name: filter_pruner_60
    epochs: [0]

  - pruner:
      instance_name: filter_pruner_40
    epochs: [0]

  - pruner:
      instance_name: filter_pruner_20
    epochs: [0]

  - extension:
      instance_name: net_thinner
    epochs: [0]

  - lr_scheduler:
      instance_name: exp_finetuning_lr
    starting_epoch: 10
    ending_epoch: 300
    frequency: 1

\end{minted}
\doublespacing

\subsection{References}
\printbibliography


\end{document}
