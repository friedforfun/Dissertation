\documentclass[../Dissertation.tex]{subfiles}

\begin{document}
\subsection{Evaluation of experimental design}
\emph{
\begin{itemize}
	\item Duration of training
	\item volume of data gathered
	\item (im)practicalities - power consumption? 
	\item limitations - single optimisation metric
	\item Criticism of methodology
\end{itemize}
}

The size of the pruned networks is not measured.



\subsection{Evaluation of results}
\emph{
\begin{itemize}
	\item Summary of results per model/dataset
	\item Deep dive into results, detailed visualisations of accuracy \& latency tradeoffs (maybe example with poor quality sensitivity analysis vs higher quality layer selection)
	\item 
\end{itemize}
}

We gathered data in 3 phases; a fast pruning phase targeting latency only with no retraining, targeting latency only with retraining, and targeting accuracy only with retraining.

\subsubsection{Experiment 1: Rapid pruning}\label{sec:FastPruningPhase}

\begin{figure}[H]
    \includegraphics[width=1\textwidth]{TargetSparsityVsLatency_No_Retrain.jpg}
    \caption{Each pruner target sparisity plotted against mean Latency per bin.}
    \label{fig:fastPruneParamVSLatency}
\end{figure}

During this phase of the experiment we gathered data to observe how pruning would effect latency, this was useful as an initial proof of concept.
This phase of the experiment was very time efficient, we were able to perform 1631 runs with around 18 hours of compute time; each run usually lasted between 24-55 seconds. 
As discussed in section (\textbf{TBD}) for this experiment we set the training epochs to 0 and set the target metric to minimize latency. 
Figure~\ref{fig:fastPruneParamVSLatency} shows the mean Latency computed by using equal width binning, where each bin represents parameter values inside each discretized 0.02 range between 0.0 and 1.0.
This chart does obfuscate any relationship between the parameters, however we can see how the filter pruner on Layer 3.2 (red) has a much greater impact on latency than the Pruner on Layer 1 (blue), this is also supported by computing the correlation between these values, see Table~\ref{tab:fastPruneCorrelations}.

\singlespacing
\begin{table}[H]
    \centering
    \begin{tabular}{@{}cp{26mm}p{26mm}p{26mm}p{26mm}@{}}
    \toprule
    \textbf{Metric}  & \textbf{Filter Pruner  Layer 1} & \textbf{Filter Pruner Layer 2} & \textbf{Filter Pruner Layer 3.1} & \textbf{Filter Pruner Layer 3.2} \\ \midrule
    \textbf{Latency} & $-0.11259$                        & $-0.552583$                      & $-0.40775$                         & $-0.80726$                         \\
    \textbf{Top1}    & $0.004462$                        & $-0.071923$                      & $-0.104505$                        & $-0.152767$                        \\ \bottomrule
    \end{tabular}
    \caption{Correlations between each target sparsity parameter and the metric being measured.}
    \label{tab:fastPruneCorrelations}
\end{table}
\doublespacing

It is interesting to note how weakly the pruning targets correlate with Top1 accuracy, this is because any pruning at all is extremely destructive to the networks accuracy. 
We found that the degree to which we prune was not at all indicative of the resulting accruacy of the network before retraining, for example we found networks with very low sparsity targets accross the board that had a much lower Top1 accuracy, than networks that were pruned with much higher targets (See models~\ref{sec:golden-sweep-523}).

\subsubsection{Experiment 2: Target Latency}

\subsubsection{Experiment 3: Target Top1}

\textbf{Interesting observations}
\begin{itemize}
    \item The models that lost all predictive power due to overpruning were not the fastest, even when targeting only latency.
    \item The relationship between more pruning and lower latency is not as simple as you get a faster model with fewer tensors
    \item When targeting accuracy we found models with as low latency when targeting latency directly.
    \item When targeting latency we found models with as high accuracy as when targeting accuracy directly.
\end{itemize}

\end{document}