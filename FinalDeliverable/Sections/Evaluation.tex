\documentclass[../Dissertation.tex]{subfiles}

\begin{document}
\subsection{Evaluation of experimental design}
\emph{
\begin{itemize}
	\item Duration of training
	\item volume of data gathered
	\item (im)practicalities - power consumption? 
	\item limitations - single optimisation metric
	\item Criticism of methodology
\end{itemize}
}

The size of the pruned networks is not measured.



\subsection{Evaluation of results}
\emph{
\begin{itemize}
	\item Summary of results per model/dataset
	\item Deep dive into results, detailed visualisations of accuracy \& latency tradeoffs (maybe example with poor quality sensitivity analysis vs higher quality layer selection)
	\item 
\end{itemize}
}

We gathered data in 3 phases; a fast pruning phase targeting latency only with no retraining, targeting latency only with retraining, and targeting accuracy only with retraining.

\subsubsection{Fast pruning phase}
During this phase of the experiment we gathered data to observe how pruning would effect latency, this was useful as an initial proof of concept.
This phase of the experiment was very time efficient, we were able to perform 1631 runs with around 18 hours of compute time, each run would usually take between 24-55 seconds. 
As discussed in section (\textbf{TBD}) for this experiment we set the training epochs to 0 and set the target metric to minimize latency. 


\textbf{Interesting observations}
\begin{itemize}
    \item The models that lost all predictive power due to overpruning were not the fastest, even when targeting only latency.
    \item The relationship between more pruning and lower latency is not as simple as you get a faster model with fewer tensors
    \item When targeting accuracy we found models with as low latency when targeting latency directly.
    \item When targeting latency we found models with as high accuracy as when targeting accuracy directly.
\end{itemize}

\end{document}