
@article{bacchusPerformanceMetricsApproximate,
  title = {Performance {{Metrics}} for {{Approximate Deep Learning}} on {{Programmable Hardware}}},
  author = {Bacchus, Denis Pascal},
  pages = {47},
  abstract = {Neural networks are increasingly used in image recognition, autonomous systems, language processing. The computational requirement for training and deploying neural networks has increased a lot over the last decade because for neural networks these application domains have grown from single hidden layer topologies with several hundred weights to deep models reaching one hundred hidden layers with millions of weights. This prohibits the use of embedded system accelerators for executing “out of the box” full precision neural networks because they have limited hardware resources for storage and computation. This is unfortunate because embedded and programmable hardware, e.g Field Programmable Gate Arrays (FPGA) are capable of performing computations extremely quickly, at a low energy cost. For neural networks, this means a high-speed classification speed, e.g classifying thousands of images each second.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\24WVBTGN\\Bacchus - Performance Metrics for Approximate Deep Learning .pdf},
  langid = {english}
}

@inproceedings{baiHighSpeedEnergy2019,
  title = {High Speed and Energy Efficient Deep Neural Network for Edge Computing},
  booktitle = {Proceedings of the 4th {{ACM}}/{{IEEE Symposium}} on {{Edge Computing}}},
  author = {Bai, Kangjun and Liu, Shiya and Yi, Yang},
  date = {2019-11-07},
  pages = {347--349},
  publisher = {{ACM}},
  location = {{Arlington Virginia}},
  doi = {10.1145/3318216.3363453},
  url = {https://dl.acm.org/doi/10.1145/3318216.3363453},
  urldate = {2020-10-01},
  abstract = {Edge computing enables data-stream acceleration with realtime data processing without latency, and allows for efficient data processing in that large amounts of data can be processed near the source with the ability to process data without ever putting it into a public cloud adds a useful layer of security for sensitive data. The edge computing-based architecture design and analysis play key impacts for the future Internet of Things (IoT) infrastructure development. In this work, we design a low power hybrid structured deep neural network (Hybrid-DNN), which employs memristive synapses working in a hierarchical information processing fashion and delay-based spiking neural network (SNN) modules as the readout layer, and provide a novel data layout method to allow the Hybrid DNN running a computationally intensive deep learning algorithm on limited resource edge devices. Motivated by the recent findings in neuromorphic computing and edge computing, we design a hybrid structured DNN combining both depth-in-space (spatial) and depth-in-time (temporal) deep learning architectures. Our Hybrid-DNN employs memristive synapses working in a hierarchical information processing fashion and delay-based spiking neural network modules as the readout layer.},
  eventtitle = {{{SEC}} '19: {{The Fourth ACM}}/{{IEEE Symposium}} on {{Edge Computing}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\H9DIBH63\\Bai et al. - 2019 - High speed and energy efficient deep neural networ.pdf},
  isbn = {978-1-4503-6733-2},
  langid = {english}
}

@article{blottFINNEndtoEndDeepLearning2018,
  title = {{{FINN}}- {{{\emph{R}}}}: {{An End}}-to-{{End Deep}}-{{Learning Framework}} for {{Fast Exploration}} of {{Quantized Neural Networks}}},
  shorttitle = {{{FINN}}- {{{\emph{R}}}}},
  author = {Blott, Michaela and Preußer, Thomas B. and Fraser, Nicholas J. and Gambardella, Giulio and O’brien, Kenneth and Umuroglu, Yaman and Leeser, Miriam and Vissers, Kees},
  date = {2018-12-22},
  journaltitle = {ACM Transactions on Reconfigurable Technology and Systems},
  shortjournal = {ACM Trans. Reconfigurable Technol. Syst.},
  volume = {11},
  pages = {1--23},
  issn = {1936-7406, 1936-7414},
  doi = {10.1145/3242897},
  url = {https://dl.acm.org/doi/10.1145/3242897},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\HI8E3LG4\\Blott et al. - 2018 - FINN- R An End-to-End Deep-Learning Framew.pdf},
  langid = {english},
  number = {3}
}

@inproceedings{bottouComparisonClassifierMethods1994,
  title = {Comparison of Classifier Methods: A Case Study in Handwritten Digit Recognition},
  shorttitle = {Comparison of Classifier Methods},
  booktitle = {Proceedings of the 12th {{IAPR International Conference}} on {{Pattern Recognition}} ({{Cat}}. {{No}}.{{94CH3440}}-5)},
  author = {Bottou, L. and Cortes, C. and Denker, J.S. and Drucker, H. and Guyon, I. and Jackel, L.D. and LeCun, Y. and Muller, U.A. and Sackinger, E. and Simard, P. and Vapnik, V.},
  date = {1994},
  volume = {2},
  pages = {77--82},
  publisher = {{IEEE Comput. Soc. Press}},
  location = {{Jerusalem, Israel}},
  doi = {10.1109/ICPR.1994.576879},
  url = {http://ieeexplore.ieee.org/document/576879/},
  urldate = {2020-10-18},
  abstract = {This paper compares the performance of several classiJier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have miscIassiJication rates less than a given threshold.},
  eventtitle = {12th {{International Conference}} on {{Pattern Recognition}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\PT3JQB2P\\Bottou et al. - 1994 - Comparison of classifier methods a case study in .pdf},
  isbn = {978-0-8186-6270-6},
  langid = {english}
}

@article{chenDeepLearningEdge2019,
  title = {Deep {{Learning With Edge Computing}}: {{A Review}}},
  shorttitle = {Deep {{Learning With Edge Computing}}},
  author = {Chen, Jiasi and Ran, Xukan},
  date = {2019-08},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {107},
  pages = {1655--1674},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2019.2921977},
  url = {https://ieeexplore.ieee.org/document/8763885/},
  urldate = {2020-10-01},
  abstract = {Deep learning is currently widely used in a variety of applications, including computer vision and natural language processing. End devices, such as smartphones and Internet-of-Things sensors, are generating data that need to be analyzed in real time using deep learning or used to train deep learning models. However, deep learning inference and training require substantial computation resources to run quickly. Edge computing, where a fine mesh of compute nodes are placed close to end devices, is a viable way to meet the high computation and low-latency requirements of deep learning on edge devices and also provides additional benefits in terms of privacy, bandwidth efficiency, and scalability. This paper aims to provide a comprehensive review of the current state of the art at the intersection of deep learning and edge computing. Specifically, it will provide an overview of applications where deep learning is used at the network edge, discuss various approaches for quickly executing deep learning inference across a combination of end devices, edge servers, and the cloud, and describe the methods for training deep learning models across multiple edge devices. It will also discuss open challenges in terms of systems performance, network technologies and management, benchmarks, and privacy. The reader will take away the following concepts from this paper: understanding scenarios where deep learning at the network edge can be useful, understanding common techniques for speeding up deep learning inference and performing distributed training on edge devices, and understanding recent trends and opportunities.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\KY7Z8NX2\\Chen and Ran - 2019 - Deep Learning With Edge Computing A Review.pdf},
  langid = {english},
  number = {8}
}

@article{chenDeepLearningMobile2020,
  title = {Deep {{Learning}} on {{Mobile}} and {{Embedded Devices}}: {{State}}-of-the-Art, {{Challenges}}, and {{Future Directions}}},
  shorttitle = {Deep {{Learning}} on {{Mobile}} and {{Embedded Devices}}},
  author = {Chen, Yanjiao and Zheng, Baolin and Zhang, Zihan and Wang, Qian and Shen, Chao and Zhang, Qian},
  date = {2020-09-26},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3398209},
  url = {https://dl.acm.org/doi/10.1145/3398209},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\BL8MAH7E\\Chen et al. - 2020 - Deep Learning on Mobile and Embedded Devices Stat.pdf},
  langid = {english},
  number = {4}
}

@article{chenDeepLearningMobile2020a,
  title = {Deep {{Learning}} on {{Mobile}} and {{Embedded Devices}}: {{State}}-of-the-Art, {{Challenges}}, and {{Future Directions}}},
  shorttitle = {Deep {{Learning}} on {{Mobile}} and {{Embedded Devices}}},
  author = {Chen, Yanjiao and Zheng, Baolin and Zhang, Zihan and Wang, Qian and Shen, Chao and Zhang, Qian},
  date = {2020-09-26},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3398209},
  url = {https://dl.acm.org/doi/10.1145/3398209},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\UV8LBZCW\\Chen et al. - 2020 - Deep Learning on Mobile and Embedded Devices Stat.pdf},
  langid = {english},
  number = {4}
}

@article{chenEyerissV2Flexible2019,
  title = {Eyeriss v2: {{A Flexible Accelerator}} for {{Emerging Deep Neural Networks}} on {{Mobile Devices}}},
  shorttitle = {Eyeriss V2},
  author = {Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S. and Sze, Vivienne},
  date = {2019-06},
  journaltitle = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  shortjournal = {IEEE J. Emerg. Sel. Topics Circuits Syst.},
  volume = {9},
  pages = {292--308},
  issn = {2156-3357, 2156-3365},
  doi = {10.1109/JETCAS.2019.2910232},
  url = {https://ieeexplore.ieee.org/document/8686088/},
  urldate = {2020-10-01},
  abstract = {A recent trend in deep neural network (DNN) development is to extend the reach of deep learning applications to platforms that are more resource and energy-constrained, e.g., mobile devices. These endeavors aim to reduce the DNN model size and improve the hardware processing efficiency and have resulted in DNNs that are much more compact in their structures and/or have high data sparsity. These compact or sparse models are different from the traditional large ones in that there is much more variation in their layer shapes and sizes and often require specialized hardware to exploit sparsity for performance improvement. Therefore, many DNN accelerators designed for large DNNs do not perform well on these models. In this paper, we present Eyeriss v2, a DNN accelerator architecture designed for running compact and sparse DNNs. To deal with the widely varying layer shapes and sizes, it introduces a highly flexible on-chip network, called hierarchical mesh, that can adapt to the different amounts of data reuse and bandwidth requirements of different data types, which improves the utilization of the computation resources. Furthermore, Eyeriss v2 can process sparse data directly in the compressed domain for both weights and activations and therefore is able to improve both processing speed and energy efficiency with sparse models. Overall, with sparse MobileNet, Eyeriss v2 in a 65-nm CMOS process achieves a throughput of 1470.6 inferences/s and 2560.3 inferences/J at a batch size of 1, which is 12.6× faster and 2.5× more energyefficient than the original Eyeriss running MobileNet.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\6KND8LRJ\\Chen et al. - 2019 - Eyeriss v2 A Flexible Accelerator for Emerging De.pdf},
  langid = {english},
  number = {2}
}

@inproceedings{chenPerformanceEvaluationEdge2018,
  title = {Performance {{Evaluation}} of {{Edge Computing}}-{{Based Deep Learning Object Detection}}},
  booktitle = {Proceedings of the 2018 {{VII International Conference}} on {{Network}}, {{Communication}} and {{Computing}} - {{ICNCC}} 2018},
  author = {Chen, Chuan-Wen and Ruan, Shanq-Jang and Lin, Chang-Hong and Hung, Chun-Chi},
  date = {2018},
  pages = {40--43},
  publisher = {{ACM Press}},
  location = {{Taipei City, Taiwan}},
  doi = {10.1145/3301326.3301369},
  url = {http://dl.acm.org/citation.cfm?doid=3301326.3301369},
  urldate = {2020-10-01},
  abstract = {This article presents a method for implementing the deep learning object detection based on a low-cost edge computing IoT device. The limit of the hardware is a challenge for working the pretrained neural network model on a low-cost IoT device. Hence, we utilize the Neural Compute Stick (NCS) to accelerate the neural network model on a low-cost IoT device by its high efficiency floating-point operation. With the NCS, the low-cost IoT device can successfully work the pre-trained neural network model and become an edge computing device. The experimental results show the proposed method can effectively detect the objects based on deep learning on an edge computing IoT device. Furthermore, the objective experiment demonstrates the proposed method can immediately infer the neural network model for images in average 1.7 seconds with only one of the NCS and the neural network model can reach average 9.2 fps for the video sequences with four NCSs acceleration. In addition, the discrepancy of the neural network model between the edge device and the edge server is less than 2\% mean average precision (mAP).},
  eventtitle = {The 2018 {{VII International Conference}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\RPMSVR79\\Chen et al. - 2018 - Performance Evaluation of Edge Computing-Based Dee.pdf},
  isbn = {978-1-4503-6553-6},
  langid = {english}
}

@article{dengTutorialSurveyArchitectures2014,
  title = {A Tutorial Survey of Architectures, Algorithms, and Applications for Deep Learning},
  author = {Deng, Li},
  date = {2014},
  journaltitle = {APSIPA Transactions on Signal and Information Processing},
  shortjournal = {APSIPA trans. signal inf. process.},
  volume = {3},
  pages = {e2},
  issn = {2048-7703},
  doi = {10.1017/atsip.2013.9},
  url = {https://www.cambridge.org/core/product/identifier/S2048770313000097/type/journal_article},
  urldate = {2020-10-16},
  abstract = {In this invited paper, my overview material on the same topic as presented in the plenary overview session of APSIPA-2011 and the tutorial material presented in the same conference [1] are expanded and updated to include more recent developments in deep learning. The previous and the updated materials cover both theory and applications, and analyze its future directions. The goal of this tutorial survey is to introduce the emerging area of deep learning or hierarchical learning to the APSIPA community. Deep learning refers to a class of machine learning techniques, developed largely since 2006, where many stages of non-linear information processing in hierarchical architectures are exploited for pattern classification and for feature learning. In the more recent literature, it is also connected to representation learning, which involves a hierarchy of features or concepts where higherlevel concepts are defined from lower-level ones and where the same lower-level concepts help to define higher-level ones. In this tutorial survey, a brief history of deep learning research is discussed first. Then, a classificatory scheme is developed to analyze and summarize major work reported in the recent deep learning literature. Using this scheme, I provide a taxonomy-oriented survey on the existing deep architectures and algorithms in the literature, and categorize them into three classes: generative, discriminative, and hybrid. Three representative deep architectures – deep autoencoders, deep stacking networks with their generalization to the temporal domain (recurrent networks), and deep neural networks (pretrained with deep belief networks) –one in each of the three classes, are presented in more detail. Next, selected applications of deep learning are reviewed in broad areas of signal and information processing including audio/speech, image/vision, multimodality, language modeling, natural language processing, and information retrieval. Finally, future directions of deep learning are discussed and analyzed.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\ZD344ABD\\Deng - 2014 - A tutorial survey of architectures, algorithms, an.pdf},
  langid = {english}
}

@article{dinelliFPGABasedHardwareAccelerator2019,
  title = {An {{FPGA}}-{{Based Hardware Accelerator}} for {{CNNs Using On}}-{{Chip Memories Only}}: {{Design}} and {{Benchmarking}} with {{Intel Movidius Neural Compute Stick}}},
  shorttitle = {An {{FPGA}}-{{Based Hardware Accelerator}} for {{CNNs Using On}}-{{Chip Memories Only}}},
  author = {Dinelli, Gianmarco and Meoni, Gabriele and Rapuano, Emilio and Benelli, Gionata and Fanucci, Luca},
  date = {2019-10-22},
  journaltitle = {International Journal of Reconfigurable Computing},
  shortjournal = {International Journal of Reconfigurable Computing},
  volume = {2019},
  pages = {1--13},
  issn = {1687-7195, 1687-7209},
  doi = {10.1155/2019/7218758},
  url = {https://www.hindawi.com/journals/ijrc/2019/7218758/},
  urldate = {2020-10-01},
  abstract = {During the last years, convolutional neural networks have been used for different applications, thanks to their potentiality to carry out tasks by using a reduced number of parameters when compared with other deep learning approaches. However, power consumption and memory footprint constraints, typical of on the edge and portable applications, usually collide with accuracy and latency requirements. For such reasons, commercial hardware accelerators have become popular, thanks to their architecture designed for the inference of general convolutional neural network models. Nevertheless, field-programmable gate arrays represent an interesting perspective since they offer the possibility to implement a hardware architecture tailored to a specific convolutional neural network model, with promising results in terms of latency and power consumption. In this article, we propose a full on-chip field-programmable gate array hardware accelerator for a separable convolutional neural network, which was designed for a keyword spotting application. We started from the model implemented in a previous work for the Intel Movidius Neural Compute Stick. For our goals, we appropriately quantized such a model through a bit-true simulation, and we realized a dedicated architecture exclusively using on-chip memories. A benchmark comparing the results on different field-programmable gate array families by Xilinx and Intel with the implementation on the Neural Compute Stick was realized. The analysis shows that better inference time and energy per inference results can be obtained with comparable accuracy at expenses of a higher design effort and development time through the FPGA solution.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\V2FAHBGB\\Dinelli et al. - 2019 - An FPGA-Based Hardware Accelerator for CNNs Using .pdf},
  langid = {english}
}

@inproceedings{drozdalTrustAutoMLExploring2020a,
  title = {Trust in {{AutoML}}: Exploring Information Needs for Establishing Trust in Automated Machine Learning Systems},
  shorttitle = {Trust in {{AutoML}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Drozdal, Jaimie and Weisz, Justin and Wang, Dakuo and Dass, Gaurav and Yao, Bingsheng and Zhao, Changruo and Muller, Michael and Ju, Lin and Su, Hui},
  date = {2020-03-17},
  pages = {297--307},
  publisher = {{ACM}},
  location = {{Cagliari Italy}},
  doi = {10.1145/3377325.3377501},
  url = {https://dl.acm.org/doi/10.1145/3377325.3377501},
  urldate = {2020-10-01},
  abstract = {We explore trust in a relatively new area of data science: Automated Machine Learning (AutoML). In AutoML, AI methods are used to generate and optimize machine learning models by automatically engineering features, selecting models, and optimizing hyperparameters. In this paper, we seek to understand what kinds of information influence data scientists’ trust in the models produced by AutoML? We operationalize trust as a willingness to deploy a model produced using automated methods. We report results from three studies – qualitative interviews, a controlled experiment, and a card-sorting task – to understand the information needs of data scientists for establishing trust in AutoML systems. We find that including transparency features in an AutoML tool increased user trust and understandability in the tool; and out of all proposed features, model performance metrics and visualizations are the most important information to data scientists when establishing their trust with an AutoML tool.},
  eventtitle = {{{IUI}} '20: 25th {{International Conference}} on {{Intelligent User Interfaces}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\EG7X6AAP\\Drozdal et al. - 2020 - Trust in AutoML exploring information needs for e.pdf},
  isbn = {978-1-4503-7118-6},
  langid = {english}
}

@article{el-sayedEdgeThingsBig2018,
  title = {Edge of {{Things}}: {{The Big Picture}} on the {{Integration}} of {{Edge}}, {{IoT}} and the {{Cloud}} in a {{Distributed Computing Environment}}},
  shorttitle = {Edge of {{Things}}},
  author = {El-Sayed, Hesham and Sankar, Sharmi and Prasad, Mukesh and Puthal, Deepak and Gupta, Akshansh and Mohanty, Manoranjan and Lin, Chin-Teng},
  date = {2018},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {6},
  pages = {1706--1717},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2017.2780087},
  url = {https://ieeexplore.ieee.org/document/8166730/},
  urldate = {2020-10-01},
  abstract = {A centralized infrastructure system carries out existing data analytics and decision-making processes from our current highly virtualized platform of wireless networks and the Internet of Things (IoT) applications. There is a high possibility that these existing methods will encounter more challenges and issues in relation to network dynamics, resulting in a high overhead in the network response time, leading to latency and traffic. In order to avoid these problems in the network and achieve an optimum level of resource utilization, a new paradigm called edge computing (EC) is proposed to pave the way for the evolution of new age applications and services. With the integration of EC, the processing capabilities are pushed to the edge of network devices such as smart phones, sensor nodes, wearables, and on-board units, where data analytics and knowledge generation are performed which removes the necessity for a centralized system. Many IoT applications, such as smart cities, the smart grid, smart traffic lights, and smart vehicles, are rapidly upgrading their applications with EC, significantly improving response time as well as conserving network resources. Irrespective of the fact that EC shifts the workload from a centralized cloud to the edge, the analogy between EC and the cloud pertaining to factors such as resource management and computation optimization are still open to research studies. Hence, this paper aims to validate the efficiency and resourcefulness of EC. We extensively survey the edge systems and present a comparative study of cloud computing systems. After analyzing the different network properties in the system, the results show that EC systems perform better than cloud computing systems. Finally, the research challenges in implementing an EC system and future research directions are discussed.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\4TMVG3HH\\El-Sayed et al. - 2018 - Edge of Things The Big Picture on the Integration.pdf},
  langid = {english}
}

@article{fukushimaNeocognitronHierarchicalNeural1988,
  title = {Neocognitron: {{A}} Hierarchical Neural Network Capable of Visual Pattern Recognition},
  shorttitle = {Neocognitron},
  author = {Fukushima, Kunihiko},
  date = {1988-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {1},
  pages = {119--130},
  issn = {08936080},
  doi = {10.1016/0893-6080(88)90014-7},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0893608088900147},
  urldate = {2020-10-18},
  abstract = {A neural network modelfor visual pattern recognition, called the "neocognitron, "' was previously proposed by the author In this paper, we discuss the mechanism of the model in detail. In order to demonstrate the ability of the neocognitron, we also discuss a pattern-recognition system which works with the mechanism o f the neocognitron.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\N4AJE4XB\\Fukushima - 1988 - Neocognitron A hierarchical neural network capabl.pdf},
  langid = {english},
  number = {2}
}

@article{fukushimaNeocognitronSelforganizingNeural1980,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  shorttitle = {Neocognitron},
  author = {Fukushima, Kunihiko},
  date = {1980-04},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  volume = {36},
  pages = {193--202},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00344251},
  url = {http://link.springer.com/10.1007/BF00344251},
  urldate = {2020-10-18},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of selforganization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cells of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\PIQ6RF8J\\Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf},
  langid = {english},
  number = {4}
}

@incollection{heAMCAutoMLModel2018,
  title = {{{AMC}}: {{AutoML}} for {{Model Compression}} and {{Acceleration}} on {{Mobile Devices}}},
  shorttitle = {{{AMC}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  volume = {11211},
  pages = {815--832},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01234-2_48},
  url = {http://link.springer.com/10.1007/978-3-030-01234-2_48},
  urldate = {2020-10-02},
  abstract = {Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-ofthe-art model compression results in a fully automated way without any human efforts. Under 4× FLOPs reduction, we achieved 2.7\% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53× on the GPU (Titan Xp) and 1.95× on an Android phone (Google Pixel 1), with negligible loss of accuracy.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\XZWC8IDP\\He et al. - 2018 - AMC AutoML for Model Compression and Acceleration.pdf},
  isbn = {978-3-030-01233-5 978-3-030-01234-2},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@online{heAMCAutoMLModel2019,
  title = {{{AMC}}: {{AutoML}} for {{Model Compression}} and {{Acceleration}} on {{Mobile Devices}}},
  shorttitle = {{{AMC}}},
  author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  date = {2019-01-15},
  url = {http://arxiv.org/abs/1802.03494},
  urldate = {2020-10-02},
  abstract = {Model compression is an effective technique facilitating the deployment of neural network models on mobile devices that have limited computation resources and a tight power budget. However, conventional model compression techniques [19, 20, 23] use hand-crafted features and require domain experts to explore the large design space trading off model size, speed, and accuracy, which is usually suboptimal and time-consuming. In this paper, we propose Automated Deep Compression (ADC) that leverages reinforcement learning in order to efficiently sample the design space and greatly improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4× FLOPs reduction, we achieved 2.7\% better accuracy than hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet and achieved a 2× reduction in FLOPs, and a speedup of 1.49× on Titan Xp and 1.65× on an Android phone (Samsung Galaxy S7), with negligible loss of accuracy.},
  archivePrefix = {arXiv},
  eprint = {1802.03494},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\KRYGZA2M\\He et al. - 2019 - AMC AutoML for Model Compression and Acceleration.pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  langid = {english},
  primaryClass = {cs}
}

@inproceedings{hochstetlerEmbeddedDeepLearning2018,
  title = {Embedded {{Deep Learning}} for {{Vehicular Edge Computing}}},
  booktitle = {2018 {{IEEE}}/{{ACM Symposium}} on {{Edge Computing}} ({{SEC}})},
  author = {Hochstetler, Jacob and Padidela, Rahul and Chen, Qi and Yang, Qing and Fu, Song},
  date = {2018-10},
  pages = {341--343},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/SEC.2018.00038},
  url = {https://ieeexplore.ieee.org/document/8567683/},
  urldate = {2020-10-01},
  abstract = {The accuracy of object recognition has been greatly improved due to the rapid development of deep learning, but the deep learning generally requires a lot of training data and the training process is very slow and complex. In this work, an Intel Movidius™ Neural Compute Stick along with Raspberry Pi 3 Model B is used to analyze the objects in the real time images and videos for vehicular edge computing. The results shown in this study tells how the stick performs in conjunction with different operating systems and processing power.},
  eventtitle = {2018 {{IEEE}}/{{ACM Symposium}} on {{Edge Computing}} ({{SEC}})},
  file = {C\:\\Users\\fried\\Zotero\\storage\\E2WQQVRF\\Hochstetler et al. - 2018 - Embedded Deep Learning for Vehicular Edge Computin.pdf},
  isbn = {978-1-5386-9445-9},
  langid = {english}
}

@article{hubelReceptiveFieldsBinocular1962,
  title = {Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex},
  author = {Hubel, D. H. and Wiesel, T. N.},
  date = {1962-01-01},
  journaltitle = {The Journal of Physiology},
  volume = {160},
  pages = {106--154},
  issn = {00223751},
  doi = {10.1113/jphysiol.1962.sp006837},
  url = {http://doi.wiley.com/10.1113/jphysiol.1962.sp006837},
  urldate = {2020-10-15},
  file = {C\:\\Users\\fried\\Zotero\\storage\\3FAJ3L3X\\Hubel and Wiesel - 1962 - Receptive fields, binocular interaction and functi.pdf},
  langid = {english},
  number = {1}
}

@article{imSparsityOptimizationFramework2004,
  title = {Sparsity: {{Optimization Framework}} for {{Sparse Matrix Kernels}}},
  shorttitle = {Sparsity},
  author = {Im, Eun-Jin and Yelick, Katherine and Vuduc, Richard},
  date = {2004-02},
  journaltitle = {The International Journal of High Performance Computing Applications},
  shortjournal = {The International Journal of High Performance Computing Applications},
  volume = {18},
  pages = {135--158},
  issn = {1094-3420, 1741-2846},
  doi = {10.1177/1094342004041296},
  url = {http://journals.sagepub.com/doi/10.1177/1094342004041296},
  urldate = {2020-10-15},
  abstract = {Sparse matrix-vector multiplication is an important computational kernel that performs poorly on most modern processors due to a low compute-to-memory ratio and irregular memory access patterns. Optimization is difficult because of the complexity of cache-based memory systems and because performance is highly dependent on the nonzero structure of the matrix. The Sparsity system is designed to address these problems by allowing users to automatically build sparse matrix kernels that are tuned to their matrices and machines. Sparsity combines traditional techniques such as loop transformations with data structure transformations and optimization heuristics that are specific to sparse matrices. It provides a novel framework for selecting optimization parameters, such as block size, using a combination of performance models and search.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\LCDXZNIV\\Im et al. - 2004 - Sparsity Optimization Framework for Sparse Matrix.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{jiangNovelDataTransformation2020,
  title = {A Novel Data Transformation and Execution Strategy for Accelerating Sparse Matrix Multiplication on {{GPUs}}},
  booktitle = {Proceedings of the 25th {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  author = {Jiang, Peng and Hong, Changwan and Agrawal, Gagan},
  date = {2020-02-19},
  pages = {376--388},
  publisher = {{ACM}},
  location = {{San Diego California}},
  doi = {10.1145/3332466.3374546},
  url = {https://dl.acm.org/doi/10.1145/3332466.3374546},
  urldate = {2020-10-02},
  abstract = {SpMM (multiplication of a sparse matrix and a dense matrix) and SDDMM (sampled dense-dense matrix multiplication) are at the core of many scientific, machine learning, and data mining applications. Because of the irregular memory accesses, the two kernels have poor data locality, and data movement overhead is a bottleneck for their performance. To overcome this issue, previous works have proposed using tiling and data reorganization to enhance data reuse. Despite their success in improving the performance for many sparse matrices, we find that the efficacy of existing techniques largely depends on how the non-zeros are distributed in a sparse matrix. In this work, we propose a novel rowreordering technique to improve data locality for SpMM and SDDMM on GPUs. The goal of such row reordering is to place similar rows close to each other, allowing them to be processed together, and thus providing better temporal locality for the values of the dense matrix. We focus on performing the row-reordering efficiently, by using a hierarchical clustering procedure optimized by locality-sensitive hashing. We also investigate when row-reordering is useful, and what factors the performance gains from our method are correlated to. Experimental evaluation using 1084 sparse matrices from SuiteSparse collection and Network Repository shows that our technique achieves up to 2.91x speedup for SpMM and up to 3.19x speedup for SDDMM against the state-of-the-art alternatives on an Nvidia P100 GPU.},
  eventtitle = {{{PPoPP}} '20: 25th {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\HP8PTGKJ\\Jiang et al. - 2020 - A novel data transformation and execution strategy.pdf},
  isbn = {978-1-4503-6818-6},
  langid = {english}
}

@inproceedings{jiangNovelDataTransformation2020a,
  title = {A Novel Data Transformation and Execution Strategy for Accelerating Sparse Matrix Multiplication on {{GPUs}}},
  booktitle = {Proceedings of the 25th {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  author = {Jiang, Peng and Hong, Changwan and Agrawal, Gagan},
  date = {2020-02-19},
  pages = {376--388},
  publisher = {{ACM}},
  location = {{San Diego California}},
  doi = {10.1145/3332466.3374546},
  url = {https://dl.acm.org/doi/10.1145/3332466.3374546},
  urldate = {2020-10-15},
  abstract = {SpMM (multiplication of a sparse matrix and a dense matrix) and SDDMM (sampled dense-dense matrix multiplication) are at the core of many scientific, machine learning, and data mining applications. Because of the irregular memory accesses, the two kernels have poor data locality, and data movement overhead is a bottleneck for their performance. To overcome this issue, previous works have proposed using tiling and data reorganization to enhance data reuse. Despite their success in improving the performance for many sparse matrices, we find that the efficacy of existing techniques largely depends on how the non-zeros are distributed in a sparse matrix. In this work, we propose a novel rowreordering technique to improve data locality for SpMM and SDDMM on GPUs. The goal of such row reordering is to place similar rows close to each other, allowing them to be processed together, and thus providing better temporal locality for the values of the dense matrix. We focus on performing the row-reordering efficiently, by using a hierarchical clustering procedure optimized by locality-sensitive hashing. We also investigate when row-reordering is useful, and what factors the performance gains from our method are correlated to. Experimental evaluation using 1084 sparse matrices from SuiteSparse collection and Network Repository shows that our technique achieves up to 2.91x speedup for SpMM and up to 3.19x speedup for SDDMM against the state-of-the-art alternatives on an Nvidia P100 GPU.},
  eventtitle = {{{PPoPP}} '20: 25th {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\ICRVR7T2\\Jiang et al. - 2020 - A novel data transformation and execution strategy.pdf},
  isbn = {978-1-4503-6818-6},
  langid = {english}
}

@inproceedings{kasturiHybridFusionLearning2020,
  title = {Hybrid {{Fusion Learning}}: {{A Hierarchical Learning Model For Distributed Systems}}},
  shorttitle = {Hybrid {{Fusion Learning}}},
  booktitle = {Proceedings of the 4th {{International Workshop}} on {{Embedded}} and {{Mobile Deep Learning}}},
  author = {Kasturi, Anirudh and Ellore, Anish Reddy and Saxena, Paresh and Hota, Chittaranjan},
  date = {2020-09-21},
  pages = {1--6},
  publisher = {{ACM}},
  location = {{London United Kingdom}},
  doi = {10.1145/3410338.3412339},
  url = {https://dl.acm.org/doi/10.1145/3410338.3412339},
  urldate = {2020-10-01},
  abstract = {Federated and fusion learning methods are state-of-the-art distributed learning approaches which enable model training without collecting private data from users. While federated learning involves lower computation cost as compared to fusion learning, the overall communication cost is higher due to a large number of communication rounds between the clients and the server. On the other hand, fusion learning reduces the overall communication cost by sending distributions of features and model parameters using only one communication round but suffers from high computation cost as it needs to find the distributions of features at the client. This paper presents hybrid fusion learning, a system that leverages hierarchical client-edge-cloud architecture and builds a deep learning model by integrating both fusion and federated learning methods. Our proposed approach uses fusion learning between the client and the edge layer to minimise the communication cost whereas it uses federated learning between the edge and the cloud layer to minimise the computation cost. Our results show that the proposed hybrid fusion learning can significantly reduce the total time taken to train the model with a small drop of around 2\% in accuracies as compared to the other two algorithms. Specifically, our results show that fusion and federated learning algorithms take up to 26.28\% and 9.74\% higher average total time to build the model, respectively, than the proposed hybrid fusion learning approach.},
  eventtitle = {{{MobiCom}} '20: {{The}} 26th {{Annual International Conference}} on {{Mobile Computing}} and {{Networking}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\TNSWR3SE\\Kasturi et al. - 2020 - Hybrid Fusion Learning A Hierarchical Learning Mo.pdf},
  isbn = {978-1-4503-8073-7},
  langid = {english}
}

@article{lecunConvolutionalNetworksImages,
  title = {Convolutional {{Networks}} for {{Images}}, {{Speech}}, and {{Time}}-{{Series}}},
  author = {LeCun, Yann and Bengio, Yoshua and Laboratories, T Bell},
  journaltitle = {The handbook of brain theory and neural networks MIT Press},
  pages = {15},
  file = {C\:\\Users\\fried\\Zotero\\storage\\QKTZN3KT\\LeCun et al. - Convolutional Networks for Images, Speech, and Tim.pdf},
  langid = {english}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {521},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  url = {http://www.nature.com/articles/nature14539},
  urldate = {2020-10-15},
  file = {C\:\\Users\\fried\\Zotero\\storage\\WFAGSZVQ\\LeCun et al. - 2015 - Deep learning.pdf},
  langid = {english},
  number = {7553}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {Nov./1998},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {86},
  pages = {2278--2324},
  issn = {00189219},
  doi = {10.1109/5.726791},
  url = {http://ieeexplore.ieee.org/document/726791/},
  urldate = {2020-10-18},
  file = {C\:\\Users\\fried\\Zotero\\storage\\73B3Y9MQ\\Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf},
  langid = {english},
  number = {11}
}

@inproceedings{liangEvolutionaryNeuralAutoML2019a,
  title = {Evolutionary Neural {{AutoML}} for Deep Learning},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Liang, Jason and Meyerson, Elliot and Hodjat, Babak and Fink, Dan and Mutch, Karl and Miikkulainen, Risto},
  date = {2019-07-13},
  pages = {401--409},
  publisher = {{ACM}},
  location = {{Prague Czech Republic}},
  doi = {10.1145/3321707.3321721},
  url = {https://dl.acm.org/doi/10.1145/3321707.3321721},
  urldate = {2020-10-01},
  abstract = {Deep neural networks (DNNs) have produced state-of-the-art results in many benchmarks and problem domains. However, the success of DNNs depends on the proper configuration of its architecture and hyperparameters. Such a configuration is difficult and as a result, DNNs are often not used to their full potential. In addition, DNNs in commercial applications often need to satisfy real-world design constraints such as size or number of parameters. To make configuration easier, automatic machine learning (AutoML) systems for deep learning have been developed, focusing mostly on optimization of hyperparameters.},
  eventtitle = {{{GECCO}} '19: {{Genetic}} and {{Evolutionary Computation Conference}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\4P3MN23G\\Liang et al. - 2019 - Evolutionary neural AutoML for deep learning.pdf},
  isbn = {978-1-4503-6111-8},
  langid = {english}
}

@inproceedings{liDetailedDataNeural2019,
  title = {The {{Detailed Data}} on the {{Neural Compute Stick Acceleration Performance}}},
  booktitle = {2019 {{Chinese Automation Congress}} ({{CAC}})},
  author = {Li, Qian and Song, Jiayu and Ning, Jiangbo and Yuan, Jianping},
  date = {2019-11},
  pages = {4959--4962},
  publisher = {{IEEE}},
  location = {{Hangzhou, China}},
  doi = {10.1109/CAC48633.2019.8996841},
  url = {https://ieeexplore.ieee.org/document/8996841/},
  urldate = {2020-10-01},
  abstract = {The four trends such as real-time, intelligence, security and privacy have spawned the rise of edge computing and front-end intelligence. The Neural Computing stick(NCS) can move calculations from the cloud to the terminal, and provide dedicated deep neural network acceleration for terminal AI devices. However, there is currently no detailed data comparison of the NCS acceleration performance. This paper compares the computational speed of convolutional neural networks on the Raspberry Pi in the presence or absence of the NCS. In addition, we further compare the calculated speeds of the CPU, GPU, and NCS through the vehicle model. The result shows that the NCS can achieve 4 to 6 times acceleration in neural networks.},
  eventtitle = {2019 {{Chinese Automation Congress}} ({{CAC}})},
  file = {C\:\\Users\\fried\\Zotero\\storage\\NSEXATJI\\Li et al. - 2019 - The Detailed Data on the Neural Compute Stick Acce.pdf},
  isbn = {978-1-72814-094-0},
  langid = {english}
}

@article{liLearningIoTEdge2018,
  title = {Learning {{IoT}} in {{Edge}}: {{Deep Learning}} for the {{Internet}} of {{Things}} with {{Edge Computing}}},
  shorttitle = {Learning {{IoT}} in {{Edge}}},
  author = {Li, He and Ota, Kaoru and Dong, Mianxiong},
  date = {2018-01},
  journaltitle = {IEEE Network},
  shortjournal = {IEEE Network},
  volume = {32},
  pages = {96--101},
  issn = {0890-8044, 1558-156X},
  doi = {10.1109/MNET.2018.1700202},
  url = {https://ieeexplore.ieee.org/document/8270639/},
  urldate = {2020-10-01},
  abstract = {Deep learning is a promising approach for extracting accurate information from raw sensor data from IoT devices deployed in complex environments. Because of its multilayer structure, deep learning is also appropriate for the edge computing environment. Therefore, in this article, we first introduce deep learning for IoTs into the edge computing environment. Since existing edge nodes have limited processing capability, we also design a novel offloading strategy to optimize the performance of IoT deep learning applications with edge computing. In the performance evaluation, we test the performance of executing multiple deep learning tasks in an edge computing environment with our strategy. The evaluation results show that our method outperforms other optimization solutions on deep learning for IoT.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\VEY2IB9C\\Li et al. - 2018 - Learning IoT in Edge Deep Learning for the Intern.pdf},
  langid = {english},
  number = {1}
}

@online{liuAutoCompressAutomaticDNN2019,
  title = {{{AutoCompress}}: {{An Automatic DNN Structured Pruning Framework}} for {{Ultra}}-{{High Compression Rates}}},
  shorttitle = {{{AutoCompress}}},
  author = {Liu, Ning and Ma, Xiaolong and Xu, Zhiyuan and Wang, Yanzhi and Tang, Jian and Ye, Jieping},
  date = {2019-09-11},
  url = {http://arxiv.org/abs/1907.03141},
  urldate = {2020-10-02},
  abstract = {Structured weight pruning is a representative model compression technique of DNNs to reduce the storage and computation requirements and accelerate inference. An automatic hyperparameter determination process is necessary due to the large number of flexible hyperparameters. This work proposes AutoCompress, an automatic structured pruning framework with the following key performance improvements: (i) effectively incorporate the combination of structured pruning schemes in the automatic process; (ii) adopt the stateof-art ADMM-based structured weight pruning as the core algorithm, and propose an innovative additional purification step for further weight reduction without accuracy loss; and (iii) develop effective heuristic search method enhanced by experience-based guided search, replacing the prior deep reinforcement learning technique which has underlying incompatibility with the target pruning problem. Extensive experiments on CIFAR-10 and ImageNet datasets demonstrate that AutoCompress is the key to achieve ultra-high pruning rates on the number of weights and FLOPs that cannot be achieved before. As an example, AutoCompress outperforms the prior work on automatic model compression by up to 33× in pruning rate (120× reduction in the actual parameter count) under the same accuracy. Significant inference speedup has been observed from the AutoCompress framework on actual measurements on smartphone. We release all models of this work at anonymous link: http://bit.ly/2VZ63dS.},
  archivePrefix = {arXiv},
  eprint = {1907.03141},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\LRHQSIBB\\Liu et al. - 2019 - AutoCompress An Automatic DNN Structured Pruning .pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@inproceedings{maMachineLearningEnabled2019,
  title = {Machine Learning Enabled Distributed Mobile Edge Computing Network},
  booktitle = {Proceedings of the 4th {{ACM}}/{{IEEE Symposium}} on {{Edge Computing}}},
  author = {Ma, Junchao and Chang, Hao-Hsuan and Fan, Pingzhi and Liu, Lingjia},
  date = {2019-11-07},
  pages = {350--351},
  publisher = {{ACM}},
  location = {{Arlington Virginia}},
  doi = {10.1145/3318216.3363454},
  url = {https://dl.acm.org/doi/10.1145/3318216.3363454},
  urldate = {2020-10-01},
  abstract = {In this work, we propose to establish a mobile edge computing (MEC) network that considers computation, caching and communication jointly. Depending on the demanding categories, users in the network are partitioned into computation-driven and cachingdriven users, both of which need memory resource to improve their quality of experiences (QoEs). Thus, a memory resource allocation problem is aroused to maximize the performance of the whole network. Due to the fact that the users’ characterization plays an important role to the resource allocation scheme and with the help of machine learning techniques, we propose to study and predict the users’ patterns by distributed learning methods which take the heterogeneity of base station type and users’ mobility, etc into consideration. The proposed machine learning based distributed MEC system can maximize the efficiency of the network by optimizing the resource allocation scheme and perfectly predicting users’ pattern.},
  eventtitle = {{{SEC}} '19: {{The Fourth ACM}}/{{IEEE Symposium}} on {{Edge Computing}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\YT85YV69\\Ma et al. - 2019 - Machine learning enabled distributed mobile edge c.pdf},
  isbn = {978-1-4503-6733-2},
  langid = {english}
}

@article{mayerScalableDeepLearning2020,
  title = {Scalable {{Deep Learning}} on {{Distributed Infrastructures}}: {{Challenges}}, {{Techniques}}, and {{Tools}}},
  shorttitle = {Scalable {{Deep Learning}} on {{Distributed Infrastructures}}},
  author = {Mayer, Ruben and Jacobsen, Hans-Arno},
  date = {2020-05-29},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3363554},
  url = {https://dl.acm.org/doi/10.1145/3363554},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\5IH7WJS9\\Mayer and Jacobsen - 2020 - Scalable Deep Learning on Distributed Infrastructu.pdf},
  langid = {english},
  number = {1}
}

@article{ningDeepReinforcementLearning2019,
  title = {Deep {{Reinforcement Learning}} for {{Vehicular Edge Computing}}: {{An Intelligent Offloading System}}},
  shorttitle = {Deep {{Reinforcement Learning}} for {{Vehicular Edge Computing}}},
  author = {Ning, Zhaolong and Dong, Peiran and Wang, Xiaojie and Rodrigues, Joel J. P. C. and Xia, Feng},
  date = {2019-12-14},
  journaltitle = {ACM Transactions on Intelligent Systems and Technology},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  volume = {10},
  pages = {1--24},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/3317572},
  url = {https://dl.acm.org/doi/10.1145/3317572},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\YNBRPL98\\Ning et al. - 2019 - Deep Reinforcement Learning for Vehicular Edge Com.pdf},
  langid = {english},
  number = {6}
}

@article{nowlanEnhancedKnowledgeDistillation,
  title = {Enhanced {{Knowledge Distillation}} for {{Neural Network Accelerators}}},
  author = {Nowlan, Andrew},
  pages = {92},
  abstract = {Recent breakthroughs in computer vision can be attributed to advances in deep learning, access to large labelled image datasets and intelligent utilisation of available hardware. Increasing demand for computer vision solutions capable of being deployed on resource constrained devices has driven a huge amount of research in both model compression and hardware design. In recent years, a new class of hardware has emerged to satisfy this demand. In particular, accelerator devices such as the Intel Movidius Myriad X VPU and the Google Coral Edge TPU have been designed specifically to accommodate deep learning workloads with reduced power requirements, providing a state-of-the-art trade off between compute performance and power consumption. Offloading deep learning inference workloads to these accelerators enables low power single board computers to run computer vision applications in real time. Even on more conventional computing platforms, these accelerators are a competitive choice of co-processor compared with typical GPUs due to their superior performance per watt. However, many state-of-theart classification models have an enormous number of parameters, making them highly computationally and resource intensive. As such, very large models are not supported by these memory constrained accelerators.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\UBDA8B3C\\Nowlan - Enhanced Knowledge Distillation for Neural Network.pdf},
  langid = {english}
}

@article{paulinoImprovingPerformanceEnergy2020,
  title = {Improving {{Performance}} and {{Energy Consumption}} in {{Embedded Systems}} via {{Binary Acceleration}}: {{A Survey}}},
  shorttitle = {Improving {{Performance}} and {{Energy Consumption}} in {{Embedded Systems}} via {{Binary Acceleration}}},
  author = {Paulino, Nuno and Ferreira, João Canas and Cardoso, João M. P.},
  date = {2020-05-29},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3369764},
  url = {https://dl.acm.org/doi/10.1145/3369764},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\8MIBZ5XX\\Paulino et al. - 2020 - Improving Performance and Energy Consumption in Em.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{pesterObjectDetectionRaspberry2019,
  title = {Object Detection with {{Raspberry Pi3}} and {{Movidius Neural Network Stick}}},
  booktitle = {2019 5th {{Experiment International Conference}} (Exp.at'19)},
  author = {Pester, Andreas and Schrittesser, Michael},
  date = {2019-06},
  pages = {326--330},
  publisher = {{IEEE}},
  location = {{Funchal (Madeira Island), Portugal}},
  doi = {10.1109/EXPAT.2019.8876583},
  url = {https://ieeexplore.ieee.org/document/8876583/},
  urldate = {2020-10-01},
  abstract = {Object detection and classification is an increasingly important field of research in machine learning. Currently, powerful GPUs (Graphics Processing Units) are used to perform the computation-intensive operations in the shortest possible computing time. However, these systems are associated with high costs. In this paper a system for object detection and classification is developed, which gets by with less resources. This should minimize the costs while keeping the performance acceptable for the target application. To keep the costs low, a Raspberry Pi3 is used as development platform in connection with a Movidius stick for the outsourcing of the ANN. After explaining the theoretical basics of object detection and ANNs, this paper shows the implementation process of the selected hardware and software. For the evaluation of this system the algorithms YOLO and MobileNet are used and pre-trained models are used as basis. Based on the MSCOCO data set, both the quality of the object classification and the computing time are evaluated.},
  eventtitle = {2019 5th {{Experiment Conference}} (Exp.at'19)},
  file = {C\:\\Users\\fried\\Zotero\\storage\\XZINHH5P\\Pester and Schrittesser - 2019 - Object detection with Raspberry Pi3 and Movidius N.pdf},
  isbn = {978-1-72813-637-0},
  langid = {english}
}

@article{pouyanfarSurveyDeepLearning2019,
  title = {A {{Survey}} on {{Deep Learning}}: {{Algorithms}}, {{Techniques}}, and {{Applications}}},
  shorttitle = {A {{Survey}} on {{Deep Learning}}},
  author = {Pouyanfar, Samira and Sadiq, Saad and Yan, Yilin and Tian, Haiman and Tao, Yudong and Reyes, Maria Presa and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, S. S.},
  date = {2019-01-23},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {51},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3234150},
  url = {https://dl.acm.org/doi/10.1145/3234150},
  urldate = {2020-10-15},
  file = {C\:\\Users\\fried\\Zotero\\storage\\J9UMB6UE\\Pouyanfar et al. - 2019 - A Survey on Deep Learning Algorithms, Techniques,.pdf},
  langid = {english},
  number = {5}
}

@incollection{scholkopfEfficientLearningSparse2007,
  title = {Efficient {{Learning}} of {{Sparse Representations}} with an {{Energy}}-{{Based Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  editor = {Schölkopf, Bernhard and Platt, John and Hofmann, Thomas},
  date = {2007},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/7503.003.0147},
  url = {https://direct.mit.edu/books/book/3168/chapter/87540/efficient-learning-of-sparse-representations-with},
  urldate = {2020-10-15},
  abstract = {We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces “stroke detectors” when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\NFMWP2YL\\Schölkopf et al. - 2007 - Efficient Learning of Sparse Representations with .pdf},
  isbn = {978-0-262-25691-9},
  langid = {english}
}

@article{szeEfficientProcessingDeep2017,
  title = {Efficient {{Processing}} of {{Deep Neural Networks}}: {{A Tutorial}} and {{Survey}}},
  shorttitle = {Efficient {{Processing}} of {{Deep Neural Networks}}},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
  date = {2017-12},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {105},
  pages = {2295--2329},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2017.2761740},
  url = {http://ieeexplore.ieee.org/document/8114708/},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\NP4DHPT5\\Sze et al. - 2017 - Efficient Processing of Deep Neural Networks A Tu.pdf},
  langid = {english},
  number = {12}
}

@article{thierry-miegHowFundamentalConcepts,
  title = {How the Fundamental Concepts of Mathematics and Physics Explain Deep Learning.},
  author = {Thierry-Mieg, Jean},
  pages = {16},
  abstract = {Starting from the Fermat’s principle of least action, which governs classical and quantum mechanics and from the theory of exterior differential forms, which governs the geometry of curved manifolds, we show how to derive the equations governing neural networks in an intrinsic, coordinate invariant way, where the loss function plays the role of the Hamitonian. To be covariant, these equations imply a layer metric which is instrumental in pretraining and explains the role of conjugation when using complex numbers. The differential formalism also clarifies the relation of the gradient descent optimizer with Aristotelian and Newtonian mechanics and why large learning steps break the logic of the linearization procedure. We hope that this formal presentation of the differential geometry of neural networks will encourage some physicists to dive into deep learning, and reciprocally, that the specialists of deep learning will better appreciate the close interconnection of their subject with the foundations of classical and quantum field theory.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\42KGY4LU\\Thierry-Mieg - How the fundamental concepts of mathematics and ph.pdf},
  langid = {english}
}

@inproceedings{tiwariNCSBasedUltra2019,
  title = {{{NCS}} Based Ultra Low Power Optimized Machine Learning Techniques for Image Classification},
  booktitle = {2019 {{IEEE Region}} 10 {{Symposium}} ({{TENSYMP}})},
  author = {Tiwari, Naman and Mondal, Koushik},
  date = {2019-06},
  pages = {750--753},
  publisher = {{IEEE}},
  location = {{Kolkata, India}},
  doi = {10.1109/TENSYMP46218.2019.8971238},
  url = {https://ieeexplore.ieee.org/document/8971238/},
  urldate = {2020-10-01},
  abstract = {In recent years there has been an extensive development in the field of convolutional neural network-based image classification because of the human-like inference results obtained, but these massive networks are resource intensive and have high memory and computational requirements. Intel’s Neural Compute Stick brings real time inference, prototyping and deployment of these DNNs to the network edge. In this paper we will discuss the development of a model for classification of book cover images into genres, and subsequently compiling the trained model for use with the Neural Compute Stick, so as to receive the optimized results in constrained environments thus ultimately leading to a system to judge a book by its cover which can be used even within a low power environment like a mobile device or Raspberry Pi, as the stick runs on power values as low as 1.2W.},
  eventtitle = {2019 {{IEEE Region}} 10 {{Symposium}} ({{TENSYMP}})},
  file = {C\:\\Users\\fried\\Zotero\\storage\\6ZRWVHNL\\Tiwari and Mondal - 2019 - NCS based ultra low power optimized machine learni.pdf},
  isbn = {978-1-72810-297-9},
  langid = {english}
}

@article{umurogluFINNFrameworkFast2017,
  title = {{{FINN}}: {{A Framework}} for {{Fast}}, {{Scalable Binarized Neural Network Inference}}},
  shorttitle = {{{FINN}}},
  author = {Umuroglu, Yaman and Fraser, Nicholas J. and Gambardella, Giulio and Blott, Michaela and Leong, Philip and Jahre, Magnus and Vissers, Kees},
  date = {2017},
  journaltitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays - FPGA '17},
  pages = {65--74},
  doi = {10.1145/3020078.3021744},
  url = {http://arxiv.org/abs/1612.07119},
  urldate = {2020-10-01},
  abstract = {Research has shown that convolutional neural networks contain significant redundancy, and high classification accuracy can be obtained even when weights and activations are reduced from floating point to binary values. In this paper, we present Finn, a framework for building fast and flexible FPGA accelerators using a flexible heterogeneous streaming architecture. By utilizing a novel set of optimizations that enable efficient mapping of binarized neural networks to hardware, we implement fully connected, convolutional and pooling layers, with per-layer compute resources being tailored to user-provided throughput requirements. On a ZC706 embedded FPGA platform drawing less than 25 W total system power, we demonstrate up to 12.3 million image classifications per second with 0.31 µs latency on the MNIST dataset with 95.8\% accuracy, and 21906 image classifications per second with 283 µs latency on the CIFAR-10 and SVHN datasets with respectively 80.1\% and 94.9\% accuracy. To the best of our knowledge, ours are the fastest classification rates reported to date on these benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1612.07119},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\BIN9MDIZ\\Umuroglu et al. - 2017 - FINN A Framework for Fast, Scalable Binarized Neu.pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Hardware Architecture,Computer Science - Machine Learning},
  langid = {english}
}

@article{verbraekenSurveyDistributedMachine2020a,
  title = {A {{Survey}} on {{Distributed Machine Learning}}},
  author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
  date = {2020-07},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  pages = {1--33},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3377454},
  url = {https://dl.acm.org/doi/10.1145/3377454},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\ME4R7E6E\\Verbraeken et al. - 2020 - A Survey on Distributed Machine Learning.pdf},
  langid = {english},
  number = {2}
}

@article{wangDeepNeuralNetwork2019a,
  title = {Deep {{Neural Network Approximation}} for {{Custom Hardware}}: {{Where We}}'ve {{Been}}, {{Where We}}'re {{Going}}},
  shorttitle = {Deep {{Neural Network Approximation}} for {{Custom Hardware}}},
  author = {Wang, Erwei and Davis, James J. and Zhao, Ruizhe and Ng, Ho-Cheung and Niu, Xinyu and Luk, Wayne and Cheung, Peter Y. K. and Constantinides, George A.},
  date = {2019-05-31},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {52},
  pages = {1--39},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3309551},
  url = {https://dl.acm.org/doi/10.1145/3309551},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\H9SXTUMX\\Wang et al. - 2019 - Deep Neural Network Approximation for Custom Hardw.pdf},
  langid = {english},
  number = {2}
}

@article{wuCompressingDeepNeural2020,
  title = {Compressing {{Deep Neural Networks With Sparse Matrix Factorization}}},
  author = {Wu, Kailun and Guo, Yiwen and Zhang, Changshui},
  date = {2020},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Netw. Learning Syst.},
  pages = {1--11},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2019.2946636},
  url = {https://ieeexplore.ieee.org/document/8901163/},
  urldate = {2020-10-02},
  abstract = {Modern deep neural networks (DNNs) are usually overparameterized and composed of a large number of learnable parameters. One of a few effective solutions attempts to compress DNN models via learning sparse weights and connections. In this article, we follow this line of research and present an alternative framework of learning sparse DNNs, with the assistance of matrix factorization. We provide an underlying principle for substituting the original parameter matrices with the multiplications of highly sparse ones, which constitutes the theoretical basis of our method. Experimental results demonstrate that our method substantially outperforms previous states of the arts for compressing various DNNs, giving rich empirical evidence in support of its effectiveness. It is also worth mentioning that, unlike many other works that focus on feedforward networks like multi-layer perceptrons and convolutional neural networks only, we also evaluate our method on a series of recurrent networks in practice.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\5L8MPPZT\\Wu et al. - 2020 - Compressing Deep Neural Networks With Sparse Matri.pdf},
  langid = {english}
}

@article{wuCompressingDeepNeural2020a,
  title = {Compressing {{Deep Neural Networks With Sparse Matrix Factorization}}},
  author = {Wu, Kailun and Guo, Yiwen and Zhang, Changshui},
  date = {2020-10},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Netw. Learning Syst.},
  volume = {31},
  pages = {3828--3838},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2019.2946636},
  url = {https://ieeexplore.ieee.org/document/8901163/},
  urldate = {2020-10-15},
  abstract = {Modern deep neural networks (DNNs) are usually overparameterized and composed of a large number of learnable parameters. One of a few effective solutions attempts to compress DNN models via learning sparse weights and connections. In this article, we follow this line of research and present an alternative framework of learning sparse DNNs, with the assistance of matrix factorization. We provide an underlying principle for substituting the original parameter matrices with the multiplications of highly sparse ones, which constitutes the theoretical basis of our method. Experimental results demonstrate that our method substantially outperforms previous states of the arts for compressing various DNNs, giving rich empirical evidence in support of its effectiveness. It is also worth mentioning that, unlike many other works that focus on feedforward networks like multi-layer perceptrons and convolutional neural networks only, we also evaluate our method on a series of recurrent networks in practice.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\6GL8KXZ2\\Wu et al. - 2020 - Compressing Deep Neural Networks With Sparse Matri.pdf},
  langid = {english},
  number = {10}
}

@article{yakovlevOracleAutoMLFast2020a,
  title = {Oracle {{AutoML}}: A Fast and Predictive {{AutoML}} Pipeline},
  shorttitle = {Oracle {{AutoML}}},
  author = {Yakovlev, Anatoly and Moghadam, Hesam Fathi and Moharrer, Ali and Cai, Jingxiao and Chavoshi, Nikan and Varadarajan, Venkatanathan and Agrawal, Sandeep R. and Idicula, Sam and Karnagel, Tomas and Jinturkar, Sanjay and Agarwal, Nipun},
  date = {2020-08},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {13},
  pages = {3166--3180},
  issn = {2150-8097},
  doi = {10.14778/3415478.3415542},
  url = {https://dl.acm.org/doi/10.14778/3415478.3415542},
  urldate = {2020-10-01},
  abstract = {Machine learning (ML) is at the forefront of the rising popularity of data-driven software applications. The resulting rapid proliferation of ML technology, explosive data growth, and shortage of data science expertise have caused the industry to face increasingly challenging demands to keep up with fast-paced develop-and-deploy model lifecycles. Recent academic and industrial research efforts have started to address this problem through automated machine learning (AutoML) pipelines and have focused on model performance as the first-order design objective. We present Oracle AutoML, a novel iteration-free AutoML pipeline designed to not only provide accurate models, but also in a shorter runtime. We are able to achieve these objectives by eliminating the need to continuously iterate over various pipeline configurations. In our feed-forward approach, each pipeline stage makes decisions based on metalearned proxy models that can predict candidate pipeline configuration performances before building the full final model. Our approach, which builds and tunes only the best candidate pipeline, achieves better scores at a fraction of the time compared to state-of-the-art open source AutoML tools, such as H2O and Auto-sklearn. This makes Oracle AutoML a prime candidate for addressing current industry challenges.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\R8UW7U2G\\Yakovlev et al. - 2020 - Oracle AutoML a fast and predictive AutoML pipeli.pdf},
  langid = {english},
  number = {12}
}


