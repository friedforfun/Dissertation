\documentclass[11pt]{article}
\usepackage[table]{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage{setspace}
\doublespacing

\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{inputenc}

\usepackage{dashrule}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{mwe}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage[backend=biber,style=ieee]{biblatex}
\usepackage[toc]{appendix}
\usepackage[acronym]{glossaries}

\addbibresource{D1.bib}

\hypersetup{ linktoc=all}
\graphicspath{ {./images/}{../images/}{../../images} }

\makenoidxglossaries

\newacronym{ai}{AI}{artifical intelligence}
\newacronym{fc}{FC}{fully connected}
\newacronym{dnn}{DNN}{deep neural network}
\newacronym{cnn}{CNN}{convolutional neural network}
\newacronym{rnn}{RNN}{recurrent neural network}
\newacronym{lstm}{LSTM}{long term short term memory network}
\newacronym{nlp}{NLP}{natural language processing}

\newacronym{ncs}{NCS}{neural compute stick}
\newacronym{tpu}{TPU}{tensor processing unit}
\newacronym{vpu}{VPU}{video processing unit}
\newacronym{gpu}{GPU}{graphics processing unit}
\newacronym{apu}{APU}{associative processing unit}
\newacronym{fpga}{FPGA}{field programmable gate array}
\newacronym{asic}{ASIC}{application specific integrated circuit}

\newacronym{blas}{BLAS}{basic linear algebra subprograms}
\newacronym{csc}{CSC}{compressed sparse column}


\usepackage{subfiles}

\begin{document}
\title{%
	\bf Inference at the edge: the impact of compression on performance\\ 
	\large Deliverable 1: Final year Dissertation \\
	Bsc Computer Science: Artificial Intelligence}

\author{
	Sam Fay-Hunt | \texttt{sf52@hw.ac.uk}\\
	Supervisor: Rob Stewart | \texttt{R.Stewart@hw.ac.uk}
}

\maketitle
\thispagestyle{empty}
\pagebreak

\textbf{DECLARATION}\\
I, Sam Fay-Hunt confirm that this work submitted for assessment is my own and is expressed in
my own words. Any uses made within it of the works of other authors in any form (e.g., ideas,
equations, figures, text, tables, programs) are properly acknowledged at any point of their
use. A list of the references employed is included.\\
Signed: ......................\\
Date: .........................
\thispagestyle{empty}
\pagebreak

\textbf{Abstract:} 
Pruning neural networks is a complex task in its own right, sucessful implementations require intricate knowledge of layer intedependencies and can be a daunting task even to the machine learning expert. 
This dissertation hypothesises that a classic hyperparameter optimisation algorithm can be applied to the domain of neural network pruning with the goal of reducing latency. 
We outline a plan to perform experiments that will highlight which pruning algorithms are most suitible to reduce inference latency.
This paper constructs a methodology to develop a framework to automate the tuning of compression parameters, and test the results. 




\thispagestyle{empty}
\pagebreak

\tableofcontents
\thispagestyle{empty}
\pagebreak

\printnoidxglossary[type=acronym, nonumberlist]
\thispagestyle{empty}

\newpage
\setcounter{page}{1}

\section{Introduction}
\subfile{Sections/introduction}

\pagebreak
\section{Background}
This Section will be split into 4 subsections:\\
Section~\ref{subsec:deepLearning} - \textbf{Deep Learning}: An overview of the basic components of a deep neural network and the \acrshort{cnn} model.\\
Section~\ref{subsec:compressionTypes} - \textbf{Neural Network Compression}: Discusses neural network compression techniques and on how they change the underlying representations of DNNs.\\
Section~\ref{subsec:AIaccelerators} - \textbf{AI accelerators} Covers a few popular AI accelerators architectures, their strengths, weaknesses and specialisms.\\
Section~\ref{subsec:hardwareArch} - \textbf{Memory factors for Deep Neural Networks}: Describes the how DNNs interact with memory, and discusses some of the implications of this.

\subsection{Deep Neural Networks}\label{subsec:deepLearning}
\subfile{Sections/Background/DeepNeuralNetworks}

\newpage
\subsection{Neural Network Compression}\label{subsec:compressionTypes}
\subfile{Sections/Background/Compression}

\newpage
\subsection{AI accelerators}\label{subsec:AIaccelerators}
\subfile{Sections/Background/AIaccelerators}

\newpage
\subsection{Memory factors for Deep Neural Networks}\label{subsec:hardwareArch}
\subfile{Sections/Background/HardwareMemArch}

\newpage
\section{Research}
\subfile{Sections/ResearchMethodology}

\newpage
\section{Preliminary Evaluation}\label{sec:prelimEval}
To demonstrate the necessity of \hyperref[obj:VerifyComp]{objective O0}, this section presents findings from a series of preliminary benchmarks. 
According to the literature covered in section~\ref{sec:Pruning} course-grained pruning algorithms should provide a demonstrable improvement in latency during inference.
Likewise given the hardware requirements quantisation is an even more consistent in its ability to reduce inference latency (see section~\ref{sec:Quantisation}).

Table~\ref{tab:PrelimResults} presents findings when benchmarking inference of resnet20 with the CIFAR10 dataset on the NCS (table ref), these results show no real change between compression methods, this is an unexpected result.

\begin{table}[H]
    \begin{tabular}{@{}|p{5cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|@{}}
    \toprule
    Compression algorithm                     & Top 1 Accuracy & Top 5 Accuracy & Latency (ms) & Throughput (FPS) \\ \midrule
    N/A (baseline)                            & 91.120         & 99.660         & 10.19        & 392.22           \\ \midrule
    AGP filter, fine-grained, and row pruning & 91.110         & 99.700         & 10.15        & 394.14           \\ \midrule
    ssl channels removal training             & 90.700         & 99.680         & 10.03        & 398.22           \\ \midrule
    ssl channels removal finetuning           & 91.610         & 99.780         & 10.17        & 389.17           \\
    \bottomrule
    \end{tabular}
    \caption{Preliminary NCS inference results, Resnet20 trained and tested with the CIFAR10 dataset.}
    \label{tab:PrelimResults}
\end{table}

\emph{Experiment stage 0} in Section~\ref{sec:Experiment0} will investigate this further with the aim of verifying proper application of the compression scheduler to the model, and assessing factors in transferring the model to the NCS.

\newpage
\section{Evaluation Strategy}
\subfile{Sections/EvaluationStrat}

\newpage
\section{Design}\label{sec:Design}
\subfile{Sections/Design}

\newpage
\section{Project Management}
\subfile{Sections/ProjectManagement}

\newpage
\appendix
\section{Back matter}
\subsection{References}
\printbibliography


\end{document}
